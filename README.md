# Link do Projeto:
https://colab.research.google.com/drive/1G1bYMqgHySTBupL3LFSdbrB8uXosiKhp?usp=sharing

# Segmenta√ß√£o de Vasos Retinianos com U-Net e U-Net++

Este projeto utiliza redes neurais convolucionais U-Net e U-Net++ para segmenta√ß√£o de vasos sangu√≠neos em imagens da retina a partir do **DRIVE Dataset**. 

O objetivo √© identificar estruturas importantes ‚Äì como vasos sangu√≠neos ‚Äì auxiliando diagn√≥sticos oftalmol√≥gicos. 

### Visualiza√ß√£o das imagens:

<img width="1215" height="574" alt="image" src="https://github.com/user-attachments/assets/ea9f2e90-8948-4011-86d9-1c229a100835" />

<img width="1212" height="584" alt="image" src="https://github.com/user-attachments/assets/560d3367-3e80-46d9-86a9-3bc5ab481165" />

Esse projeto √© composto por 40 imagens, 20 de teste e 20 de treino, o que √© pouco para um bom treinamento, por isso foi necess√°rio gerar novos dados a partir das patches, as patchs foram essenciais para esse projeto, uma vez que as imagens originaris tinham propor√ß√µes de 584x565, porem o colab tradicional limita para no m√°ximo 128x128, nessa propor√ß√£o parte da qualidade das imagens se perdiam, o que resultava em um p√©ssimo resultado.

# üìÅ Estrutura do Projeto

```
üìÇ DRIVE
 ‚îú‚îÄ‚îÄ training
 ‚îÇ   ‚îú‚îÄ‚îÄ images           # Imagens originais em .tif
 ‚îÇ   ‚îú‚îÄ‚îÄ 1st_manual       # M√°scaras manuais (r√≥tulos) em .gif
 ‚îÇ   ‚îî‚îÄ‚îÄ mask             # M√°scaras de campo de vis√£o (FOV)

 ‚îú‚îÄ‚îÄ test
 ‚îÇ   ‚îú‚îÄ‚îÄ images          
 ‚îÇ   ‚îî‚îÄ‚îÄ mask          
```
# üìÇ Organiza√ß√£o do Dataset - Kaggle para Google Drive

Este reposit√≥rio descreve como importar datasets do **Kaggle** para o **Google Drive**, mantendo uma estrutura organizada para treinamento e teste de modelos de segmenta√ß√£o como U-Net e U-Net++.

---

## üì• Como importar o dataset do Kaggle para o Google Drive

1. ### ‚úÖ Obtenha sua API key do Kaggle:
   - V√° at√© [https://www.kaggle.com](https://www.kaggle.com)
   - Clique na sua foto de perfil ‚Üí *Account*
   - Role at√© **API** e clique em **"Create New API Token"**
   - Isso far√° o download de um arquivo chamado `kaggle.json`

2. ### üîê Fa√ßa upload da chave para o seu Colab:
   No in√≠cio do notebook, execute:
   ```python
   from google.colab import files
   files.upload()  # selecione o arquivo kaggle.json
3. ### üìÅ Configure o ambiente do Kaggle no Colab:
   ```python
      !mkdir -p ~/.kaggle
      !cp kaggle.json ~/.kaggle/
      !chmod 600 ~/.kaggle/kaggle.json
4. ### ‚¨áÔ∏è Baixe o dataset desejado:
   https://www.kaggle.com/datasets/andrewmvd/drive-digital-retinal-images-for-vessel-extraction?resource=download

   ```python
      !kaggle datasets download -d aryashah2k/drive-dataset

5. ### üì¶ Extraia o conte√∫do:
   ```python
      !unzip drive-dataset.zip -d /content/drive_dataset

6. ### üîó Monte o Google Drive:
   ```python
    from google.colab import drive
    drive.mount('/content/drive')

7. ### üìÇ Organize os dados no Drive:
    Ap√≥s montar o Drive, mova os arquivos extra√≠dos:
   ```python
   !mv /content/drive_dataset /content/drive/MyDrive/DRIVE
  
## üöÄ Tecnologias Utilizadas

- Python
- TensorFlow / Keras
- NumPy
- OpenCV
- Matplotlib
- scikit-learn
- Google Colab

## ‚öôÔ∏è Pr√©-processamento

- Normaliza√ß√£o das imagens (valores entre 0 e 1).
- Binariza√ß√£o das m√°scaras: A binariza√ß√£o √© essencial para:
    Garantir que a m√°scara s√≥ indique presen√ßa (1.0) ou aus√™ncia (0.0) do objeto/estrutura de interesse.
    Facilitar o treino com modelos de segmenta√ß√£o bin√°ria, como U-Net ou SegNet.
    Evitar inconsist√™ncias nos dados vindos de arquivos de imagem.
```python
    def load_data(img_dir, mask_dir, fov_dir):
    img_files = sorted(glob.glob(os.path.join(img_dir, "*.tif")))
    mask_files = sorted(glob.glob(os.path.join(mask_dir, "*.gif")))
    fov_files = sorted(glob.glob(os.path.join(fov_dir, "*.gif")))

    images = []
    masks = []
    fov_masks = []

    for img_path, mask_path, fov_path in zip(img_files, mask_files, fov_files):
        # Carregar imagem e normalizar
        img = Image.open(img_path).convert('RGB')
        img = np.array(img, dtype=np.float32) / 255.0
        images.append(img)

        # Carregar m√°scara e binarizar
        mask = Image.open(mask_path)
        mask = np.array(mask, dtype=np.float32)
        mask[mask > 0] = 1.0 # Garantir que seja 0 ou 1
        mask = np.expand_dims(mask, axis=-1) # Adicionar canal
        masks.append(mask)

        # Carregar m√°scara de FOV e binarizar
        fov = Image.open(fov_path)
        fov = np.array(fov, dtype=np.float32)
        fov[fov > 0] = 1.0
        fov = np.expand_dims(fov, axis=-1)
        fov_masks.append(fov)

    return np.array(images), np.array(masks), np.array(fov_masks)
````

- Extra√ß√£o de patches (128x128 com stride 64): O c√≥digo divide imagens grandes em pequenos blocos (patches). S√≥ usa os patches onde h√° conte√∫do relevante (m√°scara com valores > 0), isso reduz a quantidade de dados desnecess√°rios e melhora o desempenho do modelo de segmenta√ß√£o.
  ```python
     def create_patches(images, masks, patch_size=128, stride=64):
    img_patches, mask_patches = [], []
    img_h, img_w = images.shape[1], images.shape[2]

    for i in range(images.shape[0]):
        for y in range(0, img_h - patch_size + 1, stride):
            for x in range(0, img_w - patch_size + 1, stride):
                img_patch = images[i, y:y+patch_size, x:x+patch_size]
                mask_patch = masks[i, y:y+patch_size, x:x+patch_size]
                if np.sum(mask_patch) > 0:
                    img_patches.append(img_patch)
                    mask_patches.append(mask_patch)
    return np.array(img_patches), np.array(mask_patches)
  
  X_train, X_val, y_train, y_val = train_test_split(train_images, train_masks, test_size=0.1, random_state=42)
  
  train_img_patches, train_mask_patches = create_patches(X_train, y_train)
  val_img_patches, val_mask_patches = create_patches(X_val, y_val)

- Aumento de dados com `ImageDataGenerator`.
  ### visuliza√ß√£o das imagens geradas:
  
  <img width="1213" height="634" alt="image" src="https://github.com/user-attachments/assets/bdc39b8f-9006-472e-9ce9-eb9017e43e5d" />
  <img width="1211" height="651" alt="image" src="https://github.com/user-attachments/assets/6e6dbf93-9a1e-4fec-8785-c2af8454e86d" />

## üß† Modelos Utilizados

### üî∑ U-Net

**U-Net** √© uma arquitetura de rede neural voltada para segmenta√ß√£o sem√¢ntica pixel a pixel. Ela √© composta por:

- **Encoder (contrator):** extrai caracter√≠sticas com camadas de convolu√ß√£o, ReLU e MaxPooling.
- **Decoder (expansor):** reconstr√≥i a imagem com camadas de transposed convolution.
- **Skip connections:** liga diretamente camadas correspondentes do encoder ao decoder, preservando detalhes espaciais.

> üîé √ötil para tarefas com imagens m√©dicas e dados limitados, oferece bons resultados com custo computacional moderado.
---
```python
   def build_unet(input_shape, num_classes=1):
    inputs = layers.Input(shape=input_shape)

    # Encoder
    c1 = conv_block(inputs, 64)
    p1 = layers.MaxPooling2D((2, 2))(c1)

    c2 = conv_block(p1, 128)
    p2 = layers.MaxPooling2D((2, 2))(c2)

    c3 = conv_block(p2, 256)
    p3 = layers.MaxPooling2D((2, 2))(c3)

    c4 = conv_block(p3, 512)
    p4 = layers.MaxPooling2D((2, 2))(c4)

    c5 = conv_block(p4, 1024)

    # Decoder
    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = safe_concat([u6, c4])
    c6 = conv_block(u6, 512)

    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = safe_concat([u7, c3])
    c7 = conv_block(u7, 256)

    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = safe_concat([u8, c2])
    c8 = conv_block(u8, 128)

    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = safe_concat([u9, c1])
    c9 = conv_block(u9, 64)

    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)

    model_unet= models.Model(inputs=[inputs], outputs=[outputs])
    return model
```
### üî∂ U-Net++

**U-Net++** √© uma extens√£o da U-Net, com melhorias significativas para segmenta√ß√µes mais complexas.

### Configura√ß√µes do modelo unet++
```python
import tensorflow as tf

# Fun√ß√£o de bloco de convolu√ß√£o (2x Conv2D + BatchNorm + ReLU)
def conv_block(x, filters):
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    return x

# Concatena√ß√£o segura que lida com tamanhos diferentes
def safe_concat(tensors):
    min_height = min(t.shape[1] for t in tensors)
    min_width = min(t.shape[2] for t in tensors)
    resized = [layers.Cropping2D(((0, t.shape[1] - min_height),
                                  (0, t.shape[2] - min_width)))(t)
               if t.shape[1] != min_height or t.shape[2] != min_width else t
               for t in tensors]
    return layers.Concatenate()(resized)

# Fun√ß√£o para construir a U-Net++

def build_unet_plus_plus(input_shape, num_classes=1, deep_supervision=False):
    inputs = layers.Input(shape=input_shape)
    # Encoder
    X_0_0 = conv_block(inputs, 64)
    p0 = layers.MaxPooling2D((2, 2))(X_0_0)

    X_1_0 = conv_block(p0, 128)
    p1 = layers.MaxPooling2D((2, 2))(X_1_0)

    X_2_0 = conv_block(p1, 256)
    p2 = layers.MaxPooling2D((2, 2))(X_2_0)

    X_3_0 = conv_block(p2, 512)
    p3 = layers.MaxPooling2D((2, 2))(X_3_0)

    X_4_0 = conv_block(p3, 1024)

    # Decoder
    u3_0 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(X_4_0)
    X_3_1 = conv_block(safe_concat([X_3_0, u3_0]), 512)

    u2_0 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(X_3_0)
    X_2_1 = conv_block(safe_concat([X_2_0, u2_0]), 256)

    u2_1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(X_3_1)
    X_2_2 = conv_block(safe_concat([X_2_0, X_2_1, u2_1]), 256)

    u1_0 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_0)
    X_1_1 = conv_block(safe_concat([X_1_0, u1_0]), 128)

    u1_1 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_1)
    X_1_2 = conv_block(safe_concat([X_1_0, X_1_1, u1_1]), 128)

    u1_2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_2)
    X_1_3 = conv_block(safe_concat([X_1_0, X_1_1, X_1_2, u1_2]), 128)

    u0_0 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_0)
    X_0_1 = conv_block(safe_concat([X_0_0, u0_0]), 64)

    u0_1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_1)
    X_0_2 = conv_block(safe_concat([X_0_0, X_0_1, u0_1]), 64)

    u0_2 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_2)
    X_0_3 = conv_block(safe_concat([X_0_0, X_0_1, X_0_2, u0_2]), 64)

    u0_3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_3)
    X_0_4 = conv_block(safe_concat([X_0_0, X_0_1, X_0_2, X_0_3, u0_3]), 64)

    # Deep Supervision
    if deep_supervision:
        outputs = [
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_1),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_2),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_3),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_4),
        ]
        model = models.Model(inputs, outputs)
    else:
        output = layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_4)
        model = models.Model(inputs, output)

    return model
```

#### Principais Diferen√ßas:

- **Conex√µes aninhadas e densas:** conecta m√∫ltiplos n√≠veis intermedi√°rios entre encoder e decoder.
- **Maior profundidade:** incorpora mais camadas para melhorar a extra√ß√£o sem√¢ntica.
- **Deep supervision (opcional):** supervisiona sa√≠das intermedi√°rias durante o treinamento.

> üöÄ Ideal para quando se busca maior precis√£o, mesmo com maior custo computacional.

---

## üìä Compara√ß√£o

| Caracter√≠stica             | U-Net                         | U-Net++                       |
|---------------------------|-------------------------------|-------------------------------|
| Estrutura                 | Encoder-decoder               | Encoder-decoder com conex√µes densas |
| Skip Connections          | Diretas                       | Aninhadas e profundas         |
| Aprendizado sem√¢ntico     | Razo√°vel                      | Mais refinado                 |
| Generaliza√ß√£o             | Boa                           | Melhor em dados complexos     |
| Custo computacional       | Menor                         | Maior                         |
| Suporte a deep supervision| N√£o                           | Sim (opcional)                |

---

## üìà Treinamento

- Otimizador: `Adam`
- Fun√ß√£o de perda: `binary_crossentropy`
- M√©tricas: `accuracy`, al√©m de m√©tricas personalizadas com base em segmenta√ß√£o bin√°ria
- EarlyStopping e ModelCheckpoint utilizados para evitar overfitting.

  ```python
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    checkpoint = tf.keras.callbacks.ModelCheckpoint("unetpp_drive_best.h5", save_best_only=True, monitor='val_loss')
    early = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    history = model.fit(
        train_generator, # imagens geradas pela extra√ß√£o de patchs
        steps_per_epoch=len(train_img_patches) // BATCH_SIZE, # imagens geradas com o ImageDataGenerator
        epochs=EPOCHS,
        validation_data=(val_img_patches, val_mask_patches),
        callbacks=[checkpoint, early],
    )

## üìä M√©tricas Avaliadas

As m√©tricas utilizadas para comparar os modelos foram:

- **Loss**
- **Acur√°cia**
- **Sensibilidade (Recall)**
- **Especificidade**
- **AUC-ROC**

As m√°scaras preditas passaram por p√≥s-processamento usando opera√ß√µes morfol√≥gicas (open/close) para remo√ß√£o de ru√≠dos.

## üîç Visualiza√ß√µes

O projeto inclui:

- Visualiza√ß√£o de imagens e m√°scaras originais
- Visualiza√ß√£o de imagens aumentadas
- Compara√ß√£o lado a lado entre as predi√ß√µes da U-Net e da U-Net++
- Exibi√ß√£o de predi√ß√µes binarizadas com limiar (threshold ajust√°vel)

## üß™ Avalia√ß√£o e Compara√ß√£o

Os modelos foram avaliados utilizando os patches de valida√ß√£o extra√≠dos do conjunto original.

```python
comparar_modelos(model_unet, model, val_img_patches, val_mask_patches, threshold=0.1)
```

Este m√©todo apresenta os seguintes resultados:

- **M√©dia da loss**
- **M√©dia da acur√°cia**
- **Sensibilidade m√©dia**
- **Especificidade m√©dia**
- **AUC-ROC m√©dia**

---

## üì¶ Resultados

## Unet++:

<img width="1182" height="379" alt="image" src="https://github.com/user-attachments/assets/c03a1a96-b436-4cd6-9cab-aa7bdb4424db" />

<img width="1167" height="393" alt="image" src="https://github.com/user-attachments/assets/14c8ad4d-3ea1-48fa-82c0-3b4800a86fb8" />

<img width="1555" height="534" alt="image" src="https://github.com/user-attachments/assets/48e8bee7-173b-492b-88db-e0122c20d214" />

<img width="1563" height="527" alt="image" src="https://github.com/user-attachments/assets/26e9b760-5ab7-48a7-9da4-4ffe017c30b7" />

## Unet:
<img width="1174" height="425" alt="image" src="https://github.com/user-attachments/assets/3d5bbe00-5c2c-4999-bda2-3164ab2f1896" />

<img width="1180" height="400" alt="image" src="https://github.com/user-attachments/assets/e6dbec7b-2078-4e3d-8f5d-e95ceb0c35c5" />

<img width="1567" height="534" alt="image" src="https://github.com/user-attachments/assets/e40e8e5f-915d-4bce-b30e-47e6a72336e8" />

<img width="1606" height="537" alt="image" src="https://github.com/user-attachments/assets/f2c50e18-aa13-4f5c-80c3-b74cf6c0c019" />

## üìä Resultados das M√©tricas de Avalia√ß√£o

### üîπ U-Net
- **Loss m√©dia:** 0.0973  
- **Acur√°cia m√©dia:** 0.9622  
- **Sensibilidade m√©dia:** 0.9054  
- **Especificidade m√©dia:** 0.9185  
- **AUC-ROC m√©dia:** 0.9761  

### üî∏ U-Net++
- **Loss m√©dia:** 0.0867  
- **Acur√°cia m√©dia:** 0.9660  
- **Sensibilidade m√©dia:** 0.9291  
- **Especificidade m√©dia:** 0.9263  
- **AUC-ROC m√©dia:** 0.9840  

## Conclus√£o

U-Net++ teve melhor desempenho em todas as m√©tricas, indicando que:
- Ela erra menos (menor loss),
- Classifica com maior precis√£o geral (acur√°cia),
- Tem maior sensibilidade (detecta melhor os positivos),
- Maior especificidade (detecta melhor os negativos),
- Melhor separa√ß√£o entre classes (AUC-ROC).
  
Portanto, U-Net++ √© a melhor escolha com base nesses resultados.

### Compara√ß√µa das Imagens com ambos os modelos:
<img width="1710" height="464" alt="image" src="https://github.com/user-attachments/assets/e836580d-1b74-40cc-a9db-3acde81a2d17" />
---

## üìå Observa√ß√£o

O `threshold=0.1` foi utilizado para binarizar as predi√ß√µes. Isso ajuda a captar vasos mais finos, mas deve ser ajustado conforme necess√°rio. Um valor muito baixo pode aumentar o n√∫mero de falsos positivos.

Para visualiza√ß√£o as imagens deve ter o tamanho pr√≥ximo ao original, apenas para o treinamento foi feito com propor√ß√µes 128x128.
Ao utilizar as propor√ß√µes 128x128 sem a extra√ß√£o de patchs, o resultado fica assim:

<img width="873" height="313" alt="image" src="https://github.com/user-attachments/assets/74c5e370-b546-4893-b69a-9a66104d4352" />

---

## Melhorias

Analisando o projeto, acredito que para melhor resultado seja necess√°rio fazer uma gera√ß√£o de dados mais precisa ou importar novas imagens para treinamento. 

## üìé Refer√™ncia

- Dataset: [DRIVE - Digital Retinal Images for Vessel Extraction](https://www.kaggle.com/datasets/andrewmvd/drive-digital-retinal-images-for-vessel-extraction?resource=download)
- Artigos:
  - U-Net: *Ronneberger et al., 2015*
  - U-Net++: *Zhou et al., 2018*


## üßë‚Äçüíª Autor

- **Phaola Paraguai Da Paix√£o Lustosa**
- Email: <paxaophaola@gmail.com>

---
