# Link do Projeto:
https://colab.research.google.com/drive/1G1bYMqgHySTBupL3LFSdbrB8uXosiKhp?usp=sharing

# SegmentaÃ§Ã£o de Vasos Retinianos com U-Net e U-Net++

Este projeto utiliza redes neurais convolucionais U-Net e U-Net++ para segmentaÃ§Ã£o de vasos sanguÃ­neos em imagens da retina a partir do **DRIVE Dataset**. 

O objetivo Ã© identificar estruturas importantes â€“ como vasos sanguÃ­neos â€“ auxiliando diagnÃ³sticos oftalmolÃ³gicos. 

### VisualizaÃ§Ã£o das imagens:

<img width="1398" height="674" alt="image" src="https://github.com/user-attachments/assets/dba4b11d-1588-4b60-a09d-1fc73b71aaaa" />

<img width="1027" height="498" alt="image" src="https://github.com/user-attachments/assets/d1df78e0-b25d-4573-9442-8f96317a6eac" />


Esse projeto Ã© composto por 40 imagens, 20 de teste e 20 de treino, o que Ã© pouco para um bom treinamento, por isso foi necessÃ¡rio gerar novos dados a partir das patches, as patchs foram essenciais para esse projeto, uma vez que as imagens originaris tinham proporÃ§Ãµes de 584x565, porem o colab tradicional limita para no mÃ¡ximo 128x128, nessa proporÃ§Ã£o parte da qualidade das imagens se perdiam, o que resultava em um pÃ©ssimo resultado.

# ğŸ“ Estrutura do Projeto

```
ğŸ“‚ DRIVE
 â”œâ”€â”€ training
 â”‚   â”œâ”€â”€ images           # Imagens originais em .tif
 â”‚   â”œâ”€â”€ 1st_manual       # MÃ¡scaras manuais (rÃ³tulos) em .gif
 â”‚   â””â”€â”€ mask             # MÃ¡scaras de campo de visÃ£o (FOV)

 â”œâ”€â”€ test
 â”‚   â”œâ”€â”€ images          
 â”‚   â””â”€â”€ mask          
```
# ğŸ“‚ OrganizaÃ§Ã£o do Dataset - Kaggle para Google Drive

Este repositÃ³rio descreve como importar datasets do **Kaggle** para o **Google Drive**, mantendo uma estrutura organizada para treinamento e teste de modelos de segmentaÃ§Ã£o como U-Net e U-Net++.

---

## ğŸ“¥ Como importar o dataset do Kaggle para o Google Drive

1. ### âœ… Obtenha sua API key do Kaggle:
   - VÃ¡ atÃ© [https://www.kaggle.com](https://www.kaggle.com)
   - Clique na sua foto de perfil â†’ *Account*
   - Role atÃ© **API** e clique em **"Create New API Token"**
   - Isso farÃ¡ o download de um arquivo chamado `kaggle.json`

2. ### ğŸ” FaÃ§a upload da chave para o seu Colab:
   No inÃ­cio do notebook, execute:
   ```python
   from google.colab import files
   files.upload()  # selecione o arquivo kaggle.json
3. ### ğŸ“ Configure o ambiente do Kaggle no Colab:
   ```python
      !mkdir -p ~/.kaggle
      !cp kaggle.json ~/.kaggle/
      !chmod 600 ~/.kaggle/kaggle.json
4. ### â¬‡ï¸ Baixe o dataset desejado:
   https://www.kaggle.com/datasets/andrewmvd/drive-digital-retinal-images-for-vessel-extraction?resource=download

   ```python
      !kaggle datasets download -d aryashah2k/drive-dataset

5. ### ğŸ“¦ Extraia o conteÃºdo:
   ```python
      !unzip drive-dataset.zip -d /content/drive_dataset

6. ### ğŸ”— Monte o Google Drive:
   ```python
    from google.colab import drive
    drive.mount('/content/drive')

7. ### ğŸ“‚ Organize os dados no Drive:
    ApÃ³s montar o Drive, mova os arquivos extraÃ­dos:
   ```python
   !mv /content/drive_dataset /content/drive/MyDrive/DRIVE
  
## ğŸš€ Tecnologias Utilizadas

- Python
- TensorFlow / Keras
- NumPy
- OpenCV
- Matplotlib
- scikit-learn
- Google Colab

## âš™ï¸ PrÃ©-processamento

- NormalizaÃ§Ã£o das imagens (valores entre 0 e 1).
- BinarizaÃ§Ã£o das mÃ¡scaras: A binarizaÃ§Ã£o Ã© essencial para:
    Garantir que a mÃ¡scara sÃ³ indique presenÃ§a (1.0) ou ausÃªncia (0.0) do objeto/estrutura de interesse.
    Facilitar o treino com modelos de segmentaÃ§Ã£o binÃ¡ria, como U-Net ou SegNet.
    Evitar inconsistÃªncias nos dados vindos de arquivos de imagem.
```python
    def load_data(img_dir, mask_dir, fov_dir):
    img_files = sorted(glob.glob(os.path.join(img_dir, "*.tif")))
    mask_files = sorted(glob.glob(os.path.join(mask_dir, "*.gif")))
    fov_files = sorted(glob.glob(os.path.join(fov_dir, "*.gif")))

    images = []
    masks = []
    fov_masks = []

    for img_path, mask_path, fov_path in zip(img_files, mask_files, fov_files):
        # Carregar imagem e normalizar
        img = Image.open(img_path).convert('RGB')
        img = np.array(img, dtype=np.float32) / 255.0
        images.append(img)

        # Carregar mÃ¡scara e binarizar
        mask = Image.open(mask_path)
        mask = np.array(mask, dtype=np.float32)
        mask[mask > 0] = 1.0 # Garantir que seja 0 ou 1
        mask = np.expand_dims(mask, axis=-1) # Adicionar canal
        masks.append(mask)

        # Carregar mÃ¡scara de FOV e binarizar
        fov = Image.open(fov_path)
        fov = np.array(fov, dtype=np.float32)
        fov[fov > 0] = 1.0
        fov = np.expand_dims(fov, axis=-1)
        fov_masks.append(fov)

    return np.array(images), np.array(masks), np.array(fov_masks)
````

- ExtraÃ§Ã£o de patches (128x128 com stride 64): O cÃ³digo divide imagens grandes em pequenos blocos (patches). SÃ³ usa os patches onde hÃ¡ conteÃºdo relevante (mÃ¡scara com valores > 0), isso reduz a quantidade de dados desnecessÃ¡rios e melhora o desempenho do modelo de segmentaÃ§Ã£o.
  ```python
     def create_patches(images, masks, patch_size=128, stride=64):
    img_patches, mask_patches = [], []
    img_h, img_w = images.shape[1], images.shape[2]

    for i in range(images.shape[0]):
        for y in range(0, img_h - patch_size + 1, stride):
            for x in range(0, img_w - patch_size + 1, stride):
                img_patch = images[i, y:y+patch_size, x:x+patch_size]
                mask_patch = masks[i, y:y+patch_size, x:x+patch_size]
                if np.sum(mask_patch) > 0:
                    img_patches.append(img_patch)
                    mask_patches.append(mask_patch)
    return np.array(img_patches), np.array(mask_patches)
  
  X_train, X_val, y_train, y_val = train_test_split(train_images, train_masks, test_size=0.1, random_state=42)
  
  train_img_patches, train_mask_patches = create_patches(X_train, y_train)
  val_img_patches, val_mask_patches = create_patches(X_val, y_val)

- Aumento de dados com `ImageDataGenerator`.
  ### visulizaÃ§Ã£o das imagens geradas:
  
  <img width="1213" height="634" alt="image" src="https://github.com/user-attachments/assets/bdc39b8f-9006-472e-9ce9-eb9017e43e5d" />
  <img width="1211" height="651" alt="image" src="https://github.com/user-attachments/assets/6e6dbf93-9a1e-4fec-8785-c2af8454e86d" />

## ğŸ§  Modelos Utilizados

### ğŸ”· U-Net

**U-Net** Ã© uma arquitetura de rede neural voltada para segmentaÃ§Ã£o semÃ¢ntica pixel a pixel. Ela Ã© composta por:

- **Encoder (contrator):** extrai caracterÃ­sticas com camadas de convoluÃ§Ã£o, ReLU e MaxPooling.
- **Decoder (expansor):** reconstrÃ³i a imagem com camadas de transposed convolution.
- **Skip connections:** liga diretamente camadas correspondentes do encoder ao decoder, preservando detalhes espaciais.

> ğŸ” Ãštil para tarefas com imagens mÃ©dicas e dados limitados, oferece bons resultados com custo computacional moderado.
---
```python
   def build_unet(input_shape, num_classes=1):
    inputs = layers.Input(shape=input_shape)

    # Encoder
    c1 = conv_block(inputs, 64)
    p1 = layers.MaxPooling2D((2, 2))(c1)

    c2 = conv_block(p1, 128)
    p2 = layers.MaxPooling2D((2, 2))(c2)

    c3 = conv_block(p2, 256)
    p3 = layers.MaxPooling2D((2, 2))(c3)

    c4 = conv_block(p3, 512)
    p4 = layers.MaxPooling2D((2, 2))(c4)

    c5 = conv_block(p4, 1024)

    # Decoder
    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = safe_concat([u6, c4])
    c6 = conv_block(u6, 512)

    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = safe_concat([u7, c3])
    c7 = conv_block(u7, 256)

    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = safe_concat([u8, c2])
    c8 = conv_block(u8, 128)

    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = safe_concat([u9, c1])
    c9 = conv_block(u9, 64)

    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)

    model_unet= models.Model(inputs=[inputs], outputs=[outputs])
    return model
```
### ğŸ”¶ U-Net++

**U-Net++** Ã© uma extensÃ£o da U-Net, com melhorias significativas para segmentaÃ§Ãµes mais complexas.

### ConfiguraÃ§Ãµes do modelo unet++
```python
import tensorflow as tf

# FunÃ§Ã£o de bloco de convoluÃ§Ã£o (2x Conv2D + BatchNorm + ReLU)
def conv_block(x, filters):
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    return x

# ConcatenaÃ§Ã£o segura que lida com tamanhos diferentes
def safe_concat(tensors):
    min_height = min(t.shape[1] for t in tensors)
    min_width = min(t.shape[2] for t in tensors)
    resized = [layers.Cropping2D(((0, t.shape[1] - min_height),
                                  (0, t.shape[2] - min_width)))(t)
               if t.shape[1] != min_height or t.shape[2] != min_width else t
               for t in tensors]
    return layers.Concatenate()(resized)

# FunÃ§Ã£o para construir a U-Net++

def build_unet_plus_plus(input_shape, num_classes=1, deep_supervision=False):
    inputs = layers.Input(shape=input_shape)
    # Encoder
    X_0_0 = conv_block(inputs, 64)
    p0 = layers.MaxPooling2D((2, 2))(X_0_0)

    X_1_0 = conv_block(p0, 128)
    p1 = layers.MaxPooling2D((2, 2))(X_1_0)

    X_2_0 = conv_block(p1, 256)
    p2 = layers.MaxPooling2D((2, 2))(X_2_0)

    X_3_0 = conv_block(p2, 512)
    p3 = layers.MaxPooling2D((2, 2))(X_3_0)

    X_4_0 = conv_block(p3, 1024)

    # Decoder
    u3_0 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(X_4_0)
    X_3_1 = conv_block(safe_concat([X_3_0, u3_0]), 512)

    u2_0 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(X_3_0)
    X_2_1 = conv_block(safe_concat([X_2_0, u2_0]), 256)

    u2_1 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(X_3_1)
    X_2_2 = conv_block(safe_concat([X_2_0, X_2_1, u2_1]), 256)

    u1_0 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_0)
    X_1_1 = conv_block(safe_concat([X_1_0, u1_0]), 128)

    u1_1 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_1)
    X_1_2 = conv_block(safe_concat([X_1_0, X_1_1, u1_1]), 128)

    u1_2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(X_2_2)
    X_1_3 = conv_block(safe_concat([X_1_0, X_1_1, X_1_2, u1_2]), 128)

    u0_0 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_0)
    X_0_1 = conv_block(safe_concat([X_0_0, u0_0]), 64)

    u0_1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_1)
    X_0_2 = conv_block(safe_concat([X_0_0, X_0_1, u0_1]), 64)

    u0_2 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_2)
    X_0_3 = conv_block(safe_concat([X_0_0, X_0_1, X_0_2, u0_2]), 64)

    u0_3 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(X_1_3)
    X_0_4 = conv_block(safe_concat([X_0_0, X_0_1, X_0_2, X_0_3, u0_3]), 64)

    # Deep Supervision
    if deep_supervision:
        outputs = [
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_1),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_2),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_3),
            layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_4),
        ]
        model = models.Model(inputs, outputs)
    else:
        output = layers.Conv2D(num_classes, (1, 1), activation="sigmoid")(X_0_4)
        model = models.Model(inputs, output)

    return model
```

#### Principais DiferenÃ§as:

- **ConexÃµes aninhadas e densas:** conecta mÃºltiplos nÃ­veis intermediÃ¡rios entre encoder e decoder.
- **Maior profundidade:** incorpora mais camadas para melhorar a extraÃ§Ã£o semÃ¢ntica.
- **Deep supervision (opcional):** supervisiona saÃ­das intermediÃ¡rias durante o treinamento.

> ğŸš€ Ideal para quando se busca maior precisÃ£o, mesmo com maior custo computacional.

---

## ğŸ“Š ComparaÃ§Ã£o

| CaracterÃ­stica             | U-Net                         | U-Net++                       |
|---------------------------|-------------------------------|-------------------------------|
| Estrutura                 | Encoder-decoder               | Encoder-decoder com conexÃµes densas |
| Skip Connections          | Diretas                       | Aninhadas e profundas         |
| Aprendizado semÃ¢ntico     | RazoÃ¡vel                      | Mais refinado                 |
| GeneralizaÃ§Ã£o             | Boa                           | Melhor em dados complexos     |
| Custo computacional       | Menor                         | Maior                         |
| Suporte a deep supervision| NÃ£o                           | Sim (opcional)                |

---

## ğŸ“ˆ Treinamento

- Otimizador: `Adam`
- FunÃ§Ã£o de perda: `binary_crossentropy`
- MÃ©tricas: `accuracy`, alÃ©m de mÃ©tricas personalizadas com base em segmentaÃ§Ã£o binÃ¡ria
- EarlyStopping e ModelCheckpoint utilizados para evitar overfitting.

  ```python
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    checkpoint = tf.keras.callbacks.ModelCheckpoint("unetpp_drive_best.h5", save_best_only=True, monitor='val_loss')
    early = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    history = model.fit(
        train_generator, # imagens geradas pela extraÃ§Ã£o de patchs
        steps_per_epoch=len(train_img_patches) // BATCH_SIZE, # imagens geradas com o ImageDataGenerator
        epochs=EPOCHS,
        validation_data=(val_img_patches, val_mask_patches),
        callbacks=[checkpoint, early],
    )

## ğŸ“Š MÃ©tricas Avaliadas

As mÃ©tricas utilizadas para comparar os modelos foram:

- **Loss**
- **AcurÃ¡cia**
- **Sensibilidade (Recall)**
- **Especificidade**
- **AUC-ROC**

As mÃ¡scaras preditas passaram por pÃ³s-processamento usando operaÃ§Ãµes morfolÃ³gicas (open/close) para remoÃ§Ã£o de ruÃ­dos.

## ğŸ” VisualizaÃ§Ãµes

O projeto inclui:

- VisualizaÃ§Ã£o de imagens e mÃ¡scaras originais
- VisualizaÃ§Ã£o de imagens aumentadas
- ComparaÃ§Ã£o lado a lado entre as prediÃ§Ãµes da U-Net e da U-Net++
- ExibiÃ§Ã£o de prediÃ§Ãµes binarizadas com limiar (threshold ajustÃ¡vel)

## ğŸ§ª AvaliaÃ§Ã£o e ComparaÃ§Ã£o

Os modelos foram avaliados utilizando os patches de validaÃ§Ã£o extraÃ­dos do conjunto original.

```python
comparar_modelos(model_unet, model, val_img_patches, val_mask_patches, threshold=0.1)
```

Este mÃ©todo apresenta os seguintes resultados:

- **MÃ©dia da loss**
- **MÃ©dia da acurÃ¡cia**
- **Sensibilidade mÃ©dia**
- **Especificidade mÃ©dia**
- **AUC-ROC mÃ©dia**

---

## ğŸ“¦ Resultados

## Unet++:

<img width="1182" height="379" alt="image" src="https://github.com/user-attachments/assets/c03a1a96-b436-4cd6-9cab-aa7bdb4424db" />

<img width="1167" height="393" alt="image" src="https://github.com/user-attachments/assets/14c8ad4d-3ea1-48fa-82c0-3b4800a86fb8" />

## Unet:
<img width="1174" height="425" alt="image" src="https://github.com/user-attachments/assets/3d5bbe00-5c2c-4999-bda2-3164ab2f1896" />

<img width="1180" height="400" alt="image" src="https://github.com/user-attachments/assets/e6dbec7b-2078-4e3d-8f5d-e95ceb0c35c5" />


## ğŸ“Š Resultados das MÃ©tricas de AvaliaÃ§Ã£o

### ğŸ”¹ U-Net
- **Loss mÃ©dia:** 0.0973  
- **AcurÃ¡cia mÃ©dia:** 0.9622  
- **Sensibilidade mÃ©dia:** 0.9054  
- **Especificidade mÃ©dia:** 0.9185  
- **AUC-ROC mÃ©dia:** 0.9761  

### ğŸ”¸ U-Net++
- **Loss mÃ©dia:** 0.0867  
- **AcurÃ¡cia mÃ©dia:** 0.9660  
- **Sensibilidade mÃ©dia:** 0.9291  
- **Especificidade mÃ©dia:** 0.9263  
- **AUC-ROC mÃ©dia:** 0.9840  

U-Net++ teve melhor desempenho em todas as mÃ©tricas, indicando que:
- Ela erra menos (menor loss),
- Classifica com maior precisÃ£o geral (acurÃ¡cia),
- Tem maior sensibilidade (detecta melhor os positivos),
- Maior especificidade (detecta melhor os negativos),
- Melhor separaÃ§Ã£o entre classes (AUC-ROC).
  
Portanto, U-Net++ Ã© a melhor escolha com base nesses resultados.

### ComparaÃ§Ãµa das Imagens de ambos os modelos:
<img width="1710" height="464" alt="image" src="https://github.com/user-attachments/assets/e836580d-1b74-40cc-a9db-3acde81a2d17" />
---

## ğŸ“Œ ObservaÃ§Ã£o

O `threshold=0.1` foi utilizado para binarizar as prediÃ§Ãµes. Isso ajuda a captar vasos mais finos, mas deve ser ajustado conforme necessÃ¡rio. Um valor muito baixo pode aumentar o nÃºmero de falsos positivos.

Para visualizaÃ§Ã£o as imagens deve ter o tamanho prÃ³ximo ao originais, apenas para o treinamento, foi feito com proporÃ§Ãµes 128x128.
Ao utilizar as proporÃ§Ãµes 128x128 sem a extraÃ§Ã£o de patchs, o resultado fica assim:

<img width="873" height="313" alt="image" src="https://github.com/user-attachments/assets/74c5e370-b546-4893-b69a-9a66104d4352" />

---

## ğŸ“ ReferÃªncia

- Dataset: [DRIVE - Digital Retinal Images for Vessel Extraction](https://www.kaggle.com/datasets/andrewmvd/drive-digital-retinal-images-for-vessel-extraction?resource=download)
- Artigos:
  - U-Net: *Ronneberger et al., 2015*
  - U-Net++: *Zhou et al., 2018*


## ğŸ§‘â€ğŸ’» Autor

- **Phaola Paraguai Da PaixÃ£o Lustosa**
- Email: <paxaophaola@gmail.com>

---
